Stats 744: Lecture Notes

Sep 10, 2019: Scales and How humans visualize data
	-other scales are just as useful (like log) as linear scales
	-if data is related or anchored to our data then we can use this to think about how we want to show our data, should it be ratios?
	-sometimes we have another natural anchor that is not zero
	-so sometimes if our data has zero on the graph but our data is no where near zero then it makes the slope  or whatever trends hard to see
	-girdlines are important bc it lets us see the trends more clearly bc it gives us a bit of a reference point
	-you really can make you data look like your story supports whatever you are suposed to say
	-can choose an anchor so some arbitrary line you choose showing an average or whatever so you can compare the rest of your data to that
	-and choosing an anchor is a scientific decision, this matters in the message you send, but more importantly does it convey accurate info
	-it is super easy to be misleading with your figures, espically when you try to assoicate a physical area to values but inaccuratly
	-it is ok to use area if you are comparing fairly, so its a physical quantity, num of something, but the ratios are correct. so you anchor to 0 or whatever acutually makes sense. position on shared access is proportional
	-log scale is often good for physical quantities, like where zero means zero and you dont have a negative
	-always make sure that the scale is more relevant for the scientific accuracy or the story you want to tell
	-points and error bars are usually better than bar graphs
	-differences between transforming your data or just putting your data on a log scale. one changes your points and the other just changes the axis so that points are closer or farther or whatever
	-when we are dealing with probability we should be dealing with the log odds scale, bc small probabilities should look bigger than big probabilities
	-so for showing you data its usually ok for visualization to transform your data, but if you are doing stats on that then you should usually NOT transform your data
	-if you have extreme values after transforming ggplot will put your extreme values ON the axis line 
	-otherwise, if you do not use ggplot then you should just increase your scale so use log(1+x) instead of log(x)
	-so with odds it is basically saying this doubles instead of increases by x amount
	-it is therefore a natural place to think about differences (in some cases, when we are looking at probabilities)
	-we do this so that we can always put things into a linear distance
	
Sep 13, 2019: History of Data Viz
	-Tukey, super important in some basic stats that ppl use all the time
	-with big datasets, then there will have lots of outliers just bc, so its not necessarally an issue
	-bag plot is similar to a box plot but shows it in a more robust way
	-so even with the age of comps we still like Tukeys quick ways to look at our data bc we are always getting more data always so we still need to do things computationally efficently
	-Cleveland showing ppl the same data but in 2 diff graphs and then asked ppl to make inferences about the data. found that ppl are better at looking at position rather than angles (goes back to stuff from video)
	-so Cleveland was good bc he actually tested how good ppl are at differentiating instead of just saying "I feel"
	-Mechanical Turk (getting ppl to do your tests across the world, and get paid very little per question)
	-Tufte has a lot of strong opinions, basically think that you should maximize info in your graph and really saying is this conveying info or not
	-everything about the figure should be in the legened or in caption, if its in the text...its hard for ppl to find and therefore understand
	-so an example of Tufte not wanting all the extra stuff you can put it in a tufte boxplot where there is no "box" but this really gives minimal minimal picture
	-its also culture bc some fields may be used to a certain type of graph, which can make it hard to decide if that graph is wrong..what do you use? things to think about
	-putting your legend lables are sometimes better to put lable actually on graph in the same colour as you are using, ggplot does this automatically (direct lables package)
	-ggrepel will automatically push points and lables to empty parts of the graph so you can see them
	-the most important thing is reproducibility, so other ppl can do what you also want to do
	-so editing manually like in illustrator or something. 
	-nice set of rules to decide what you should put on what axis and how to use colours...etc
	-y-axis is most important!
	-want to put the most numbers/categories of something on the x-axis then put the only 2 treatments or whatever as colours bc its easier for ppl to distinguish 2 colours than 10, so the 10 would go on the x-axis
	-if you have 2 categories that are equal, like MF and 2 temp categories do you put them on the x-axis or in colours? it depends on what you are most interested in. so whatever you are more interested in should always be on the legend. so its easier to compare within groups than btwn groups bc they are physically closer.
	-so what you are interested in should go as close together as possible.
	-facet_wrap : assumes that you have one categorical variable but with LOADS of categories, ggplot will then put all your x categories onto the same graph, and usually puts it left to right, top to bottom like how ppl are used to reading
	-facet_grid : will arrange it as groups, so rows are one group and cols are another group
	-so how do you choose?
	-so if you want to compare across A then its easier to compare across rows so facet_grid would be better
	-but if you want to compare A and B then it may be better to do facet_wrap
	-transparent is always good too so you can make the most important stuff visually eye catching.
	-if you need to know what group something is in v.s. if you just need to know they are different also impacts how you should pick colours or groupings
	-the computers defaults of ordering is usually bad
	-do you want to show means and SD and distribution? or do you want to show inferrential differences so just error and point?
	-usually you should not combine those bc they are often on diff scales
	-so its always picking what you are more interested in and going with that, if you can't fit all your stuff onto one graph
	-notches add in an inferrential overlay onto a boxplot that is showing discriptive stuff. so a way to sort of combine both, but does not work with small dataset
	-3D makes it really hard for us to tell if one thing is taller than another bc perspective is hard. so avoide it if you can

Sep 17, 2019: Exploratory Graphics
	-be careful about how you are looking at your data bc your desires can influence how you do your analysis
	-be careful bc if you have enough, then stuff can be sig by chance!
	-look at predictor and response variables separately so you can look at whats up without making too many weird assumptions
	-so if you get other ideas while you are looking at your data about other things you can look at, just make sure you lable it as exploratory so that you wont be making assumptions on something that was not collected or implemented properly
	-with individual variables we often want to look at the shape of that data so we can then decide what tests to do on the data
	-think about weather we want to make parametric or non-parametric assumptions. so goes back to wheather we show the means and SE or the distribution. so this will require different tests
	-plotting standard deviation means that you are making a non-parametirc assumption, SE is parametric assumption
	-this is where log vs linear scale is important bc it will impact what assumptions you make
	-so in the ex when you analize the data, if your SD are more similar across samples or whatever then it will be better to do analysis on later, so log scale is good in this case
	-think about it biologically and what you should see and if that makes sense with your scale
	-so all the classics, box plots, violin plots, SD and mean, SE and mean...etc are good ways to explore
	-if all your values are going to stay positive consider maybe doing a log scale bc they will always stay positive
	-so again depending on what you are looking at, arithmatic mean vs geometric mean may be easier to understand and may be better to convey your point
	-geometric mean = mutiplying, arithmatic = adding
	-so if you are interested in ppls behavious you may want to look at this on a log scale so that you can tell better. vs. if you are worried about making money then you should be looking at it on a linear scale.
	-can change the width of your box plots to be the width of how much data you have in each category bc then its really easy to see that you have less data in a certain category and therefore maybe cant make any inferences bc there is just not enought data
	-get things into a human readable form thats the best
	-so if your points jump in order of mag a lot then log is probs better than normal data
	-remember on the log scale that if you say log10 particles per micro l is 3 times more, is NOT a physical value so its not actually 3 times more bc its on a LOG scale, which bring these numbers closer than they actually are
	-visually you do not want things to be too spikey, bc it is harder to tell whats up, bc humans are better at seeing trends when it is closest to the 45deg angle
	-try messing with the aspect ratio so ratio of xaxis scale vs yaxis scale, to try to play with this to make your data clear
	-lattice in r will automatically make the aspect ratio nicely, possibly in ggplot but not sure how automatic it is
	-with lots of noisy data, having a smooth line on top is good so that you can see the trend underneath
	-scatterplots are good and go to for scatter plots
	-if you have lots of points you want to add on some sort of interpolation so you can see it
	-transparancy is good to show density if you have more data in one section on another
	-scale size area is also good to show density of the area

Sep 20, 2019: Exploratory Graphics:
	-Anscombe's quartet: 4 x-y data plots that all have the same mean, covariance, ...etc. so they are very easy to combine. all the standard bivariate summary stats are all the same btwn the plots. so its easy to compare
	-so this group of graphs is a really useful way to look at the data to get the general idea about your data and what it looks like and is useful so you dont miss any weird trends. better than doing a lot of tests to check for all the things you can think of
	-scatter plots are generally good, trendlines are usually good when you have too much data
	-confidence intervals are infferential NOT discriptive, bc they are tlaking about the line you drew not the data itself. if we want to see the distribution of the data, we need to do something else
	-violin plots are supposed to be representing the probability density, so you are changing a bunch of points to a distribution/continuous density, it does this by computing the kernal density estimator, which takes a set of points and sum a typically symmertic and smoot function (k) that decreases with distance. (so like a bell shaped curve, gaussian distribution). Then you get a kernal density estimate at a give point x, is the sum over all the locations of the kernal value at that distance. sort of like a histogram but with a sliding window. but they make it smooth off the bat so the output is a smooth non-parametric results.
	-so just like in histograms you need to choose the bin width correctly, the kernal also has a similar issue that you have to choose the bandwidth (width of the gaussian curve) that is good to summarize the data. but both of these are done automatically and huristically
	-so this is why when we do violin plots on not very many points we get weird bumpy violin plots.
	-geom_density_2d() is a 2D kernal to the density, so you get these contour graphs
	-defalt gor this is not super human readable, so the density colours are hard to show bc ppl do not understand the "density levels", also the default colours are shit and makes it harder to compare
	-so with contours you want the inner sections to be more prominant compared to the outter edges so that they look more important and more dense so that there is no misinterpretation.
	-hurrican and trump example (https://eeecon.uibk.ac.at/~zeileis//news/dorian_rainbow/)
	-can put different categories having the contours ontop of eachother, but you should choose fewer contours or use facet, but again these all come with pros and cons
	-also have to think about colour blind and how it would look
	-for big datasets you can use hexes (geom_hex()) and it will go faster
	-for hexes its counts per hex, and its nice bc it allows you to see all the datapoints where as with density it may be missed
	-same general question as how many bars you put in a histogram, you can also choose the number of hexes carefully
	-always the option to manually change the size of the bins/hexes/kernal stuff
	
	Colour:
		-use clear gradients
		-if the map contrasts with the background, zero should match the background
		-if zero has a physical meaning, then go in one direction, red -> white, light -> dark, NOT blue -> white <- blue (no colour in the middle)
		-but if there is a natural "middle" then you can use blue -> white <- red (colour in the middle)
	
	Multi-Variate:
		- if you have lots of data, pairs plots are useful, but they only show the pairwise plots, but this is a good start
		-so you can see how your diff categories are correlated, good summary of the data
		-GGally: similar to ggpairs, so with factors it will do bar plot instead of kernal density plots, and instead of scatter plots, it will make box plots for factors, good overview of the whole dataset
		-double decker plots, spine plots, mosaic plots: for highly multivariate NON-continuous data (categorical)
		-it helps us show and understand the relationships btwn the datapoints, only when its continuous
	
	Diagnostic Graphics:
		-it is important to not do any "snooping" when you are making assumptions before you see if you data fits that model or assumptions or whatever
		-IRL our datasets are never really fitting into our "normal distributions"...etc
		-all we care about is that our data is close enought to normal that our models are ok and good and fine
		-broom package in R, is the extended from tidyverse, i
		-tidy: will give you a coefficent table for a bunch of models in a consistant format with consistant commands, supposed to be unified
		-augment: gives you predictons and inference measures
		-glance: summary of the model
		-so you do all these after you fit your model fo you can look at stuff and plot things
		-qqplots tell us what kind of distribution your data is, heteroskedastic, fat tailed, thin tails...etc
		-so the observed values are y-axis, and expected are on the x. so we can look and see how it would fit with the 1-1 line.
		-so in this example we see that there are fat tails and its symmertical
		-always want to be looking at the distribution of the residuals
		-when fitting a model we need to worry about these things in order:
			-mean model (bias)
			-variance model (heteroscedasticity)
			-distribution (non-normal..etc)
		-residuals should have a constant mean zero
		-you should be looking at the patterns in the residuals among the different categories to see whats wrong and you can fix the model
		-did you leave out covariates and interactions? if its not linear, dont fit linear.
		-think about using polynomal or splines to try and fix non-linear data
		-heteroscidacity: when we plot residuals v fitted we should see a fan. this dosent really work if we have sampling errors or too many points
		-so to fix this we do the square root of fitted and residuals to check and fix those things, here you want the graph to be flat
		-low est plot? they can be wiggly but its usually fine
		-should not use linear reg when doing diagnostics bc you want to see all the variation and weirdness so you can fix it
		-box-cox: we have decided we want to transform the data and its not going to cause problems. so this is a family of transformations (power transformations) x to the power of lambda (where lambda can vary, obviously), so as lambda goes to zero it is a log transformation. so it draws a curve of the goodness of fit or likelyhood of data with confidence intervals of the best lambda so you can pick what transformation you should do
		-this box-cox is said to be a rough guide for you to pick a whole number of lambda (in MASS:boxcox package), bc whole numbers are easier to explain and justify
		-box-cox only handles POSITIVE data
		-can calculate an autocorrelation plot which gives the distribution of the residuals over whatever scale you have (so time or along a chrom or whatever, something with a scae)
		-can group the data from several residuals to get some goodness of fit for a log reg
		-it takes fitted and residuals and groups the data for each of those and tries to see if there is variability of not
		-so you basically bin residuals and it gives you numbers that are a bit easier to interpret
		-can do the same thing but break the data into subsets of categories you already have so that you can then look at the residuals and see whats going on.
		-basically you want to break things down to see if there are patterns that you may be missing
		
		-likelihood profiles: transforming curves to a straight line so you can see if it is quadratic or not
		-if you can see a trend in a trace plot then thats bad, bc your MCMC is not efficently sampling your paramaters
		-trace plots should look like just a band of up and down noise, no trends/patterns

	Sep 24, 2019:What is happening when we do smooth lines
		-loess is like the kernal smoothing, but is essentially fitting a quadratic regression on little pieces of the data, and like kernal smoothing its weighting and sliding the window. weights the points near the middle of the window more
		-computationaly expensive
		-O(N^2) computational notation that says the comp time increases to the number of data points to the squared. so comp time increases with increased num of points
		-has a span thing that is similar to choosing the right bind width...etc, so you can change this to make the curve less or more wiggly
		-mostly defaults are ok
		-if you dont like the results, we usually switch. GAM, additive model, poisson....etc
		-so geom_smooth( method = loess) or change it to whatever, geom has a default but we should specify it always 
		-always make sure your warnings go away bc otherwise you are probs doing something wrong!
		-mgcv fits way too many splices but then is figuring out how wiggly it is and then peanalizes the coefficents so that it brings back to a smooth curve, so the idea with this is that it is a more robst way to choose a curve
		-sometimes with GAM you get lots of lines, which is usually bc it guessed wrong on the number of knots or bins
		-so always just specify everything so that you dont get any warnings
		-with GAMs you can tell it to use a poisson or least squares or whatever, so this is good if you have values that are going below zero when really you should not have that
		-a good way to see whats up with your data is to plot all your x values against each y value? (see multiple continuous predictors, single response)
		-pivot_longer is new and in tidy and a nice way to re-shape your data
		-good way to get a sense of what is happening with all the univariate stuff (so cylinders, carbs, ...etc in the cars dataset) ^
		-again this does not show interactions with other variables, just whats up

		-library "sos" is good for trying to find a command or package or something like that. see lectre notes
		-geom_path puts the points in the order they were in the dataset, where as genom_line assumes you x axis is the predictor variable, so you have to order your points based on the x and y var (so genotype data first, and then environment in the lecture notes)
		-group =1 means that you can do points in diff colours of your groups, and then put a smooth genom line over all the groups if group =1 
		-
