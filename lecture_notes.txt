Stats 744: Lecture Notes

Sep 10, 2019: Scales and How humans visualize data
	-other scales are just as useful (like log) as linear scales
	-if data is related or anchored to our data then we can use this to think about how we want to show our data, should it be ratios?
	-sometimes we have another natural anchor that is not zero
	-so sometimes if our data has zero on the graph but our data is no where near zero then it makes the slope  or whatever trends hard to see
	-girdlines are important bc it lets us see the trends more clearly bc it gives us a bit of a reference point
	-you really can make you data look like your story supports whatever you are suposed to say
	-can choose an anchor so some arbitrary line you choose showing an average or whatever so you can compare the rest of your data to that
	-and choosing an anchor is a scientific decision, this matters in the message you send, but more importantly does it convey accurate info
	-it is super easy to be misleading with your figures, espically when you try to assoicate a physical area to values but inaccuratly
	-it is ok to use area if you are comparing fairly, so its a physical quantity, num of something, but the ratios are correct. so you anchor to 0 or whatever acutually makes sense. position on shared access is proportional
	-log scale is often good for physical quantities, like where zero means zero and you dont have a negative
	-always make sure that the scale is more relevant for the scientific accuracy or the story you want to tell
	-points and error bars are usually better than bar graphs
	-differences between transforming your data or just putting your data on a log scale. one changes your points and the other just changes the axis so that points are closer or farther or whatever
	-when we are dealing with probability we should be dealing with the log odds scale, bc small probabilities should look bigger than big probabilities
	-so for showing you data its usually ok for visualization to transform your data, but if you are doing stats on that then you should usually NOT transform your data
	-if you have extreme values after transforming ggplot will put your extreme values ON the axis line 
	-otherwise, if you do not use ggplot then you should just increase your scale so use log(1+x) instead of log(x)
	-so with odds it is basically saying this doubles instead of increases by x amount
	-it is therefore a natural place to think about differences (in some cases, when we are looking at probabilities)
	-we do this so that we can always put things into a linear distance
	
Sep 13, 2019: History of Data Viz
	-Tukey, super important in some basic stats that ppl use all the time
	-with big datasets, then there will have lots of outliers just bc, so its not necessarally an issue
	-bag plot is similar to a box plot but shows it in a more robust way
	-so even with the age of comps we still like Tukeys quick ways to look at our data bc we are always getting more data always so we still need to do things computationally efficently
	-Cleveland showing ppl the same data but in 2 diff graphs and then asked ppl to make inferences about the data. found that ppl are better at looking at position rather than angles (goes back to stuff from video)
	-so Cleveland was good bc he actually tested how good ppl are at differentiating instead of just saying "I feel"
	-Mechanical Turk (getting ppl to do your tests across the world, and get paid very little per question)
	-Tufte has a lot of strong opinions, basically think that you should maximize info in your graph and really saying is this conveying info or not
	-everything about the figure should be in the legened or in caption, if its in the text...its hard for ppl to find and therefore understand
	-so an example of Tufte not wanting all the extra stuff you can put it in a tufte boxplot where there is no "box" but this really gives minimal minimal picture
	-its also culture bc some fields may be used to a certain type of graph, which can make it hard to decide if that graph is wrong..what do you use? things to think about
	-putting your legend lables are sometimes better to put lable actually on graph in the same colour as you are using, ggplot does this automatically (direct lables package)
	-ggrepel will automatically push points and lables to empty parts of the graph so you can see them
	-the most important thing is reproducibility, so other ppl can do what you also want to do
	-so editing manually like in illustrator or something. 
	-nice set of rules to decide what you should put on what axis and how to use colours...etc
	-y-axis is most important!
	-want to put the most numbers/categories of something on the x-axis then put the only 2 treatments or whatever as colours bc its easier for ppl to distinguish 2 colours than 10, so the 10 would go on the x-axis
	-if you have 2 categories that are equal, like MF and 2 temp categories do you put them on the x-axis or in colours? it depends on what you are most interested in. so whatever you are more interested in should always be on the legend. so its easier to compare within groups than btwn groups bc they are physically closer.
	-so what you are interested in should go as close together as possible.
	-facet_wrap : assumes that you have one categorical variable but with LOADS of categories, ggplot will then put all your x categories onto the same graph, and usually puts it left to right, top to bottom like how ppl are used to reading
	-facet_grid : will arrange it as groups, so rows are one group and cols are another group
	-so how do you choose?
	-so if you want to compare across A then its easier to compare across rows so facet_grid would be better
	-but if you want to compare A and B then it may be better to do facet_wrap
	-transparent is always good too so you can make the most important stuff visually eye catching.
	-if you need to know what group something is in v.s. if you just need to know they are different also impacts how you should pick colours or groupings
	-the computers defaults of ordering is usually bad
	-do you want to show means and SD and distribution? or do you want to show inferrential differences so just error and point?
	-usually you should not combine those bc they are often on diff scales
	-so its always picking what you are more interested in and going with that, if you can't fit all your stuff onto one graph
	-notches add in an inferrential overlay onto a boxplot that is showing discriptive stuff. so a way to sort of combine both, but does not work with small dataset
	-3D makes it really hard for us to tell if one thing is taller than another bc perspective is hard. so avoide it if you can

Sep 17, 2019: Exploratory Graphics
	-be careful about how you are looking at your data bc your desires can influence how you do your analysis
	-be careful bc if you have enough, then stuff can be sig by chance!
	-look at predictor and response variables separately so you can look at whats up without making too many weird assumptions
	-so if you get other ideas while you are looking at your data about other things you can look at, just make sure you lable it as exploratory so that you wont be making assumptions on something that was not collected or implemented properly
	-with individual variables we often want to look at the shape of that data so we can then decide what tests to do on the data
	-think about weather we want to make parametric or non-parametric assumptions. so goes back to wheather we show the means and SE or the distribution. so this will require different tests
	-plotting standard deviation means that you are making a non-parametirc assumption, SE is parametric assumption
	-this is where log vs linear scale is important bc it will impact what assumptions you make
	-so in the ex when you analize the data, if your SD are more similar across samples or whatever then it will be better to do analysis on later, so log scale is good in this case
	-think about it biologically and what you should see and if that makes sense with your scale
	-so all the classics, box plots, violin plots, SD and mean, SE and mean...etc are good ways to explore
	-if all your values are going to stay positive consider maybe doing a log scale bc they will always stay positive
	-so again depending on what you are looking at, arithmatic mean vs geometric mean may be easier to understand and may be better to convey your point
	-geometric mean = mutiplying, arithmatic = adding
	-so if you are interested in ppls behavious you may want to look at this on a log scale so that you can tell better. vs. if you are worried about making money then you should be looking at it on a linear scale.
	-can change the width of your box plots to be the width of how much data you have in each category bc then its really easy to see that you have less data in a certain category and therefore maybe cant make any inferences bc there is just not enought data
	-get things into a human readable form thats the best
	-so if your points jump in order of mag a lot then log is probs better than normal data
	-remember on the log scale that if you say log10 particles per micro l is 3 times more, is NOT a physical value so its not actually 3 times more bc its on a LOG scale, which bring these numbers closer than they actually are
	-visually you do not want things to be too spikey, bc it is harder to tell whats up, bc humans are better at seeing trends when it is closest to the 45deg angle
	-try messing with the aspect ratio so ratio of xaxis scale vs yaxis scale, to try to play with this to make your data clear
	-lattice in r will automatically make the aspect ratio nicely, possibly in ggplot but not sure how automatic it is
	-with lots of noisy data, having a smooth line on top is good so that you can see the trend underneath
	-scatterplots are good and go to for scatter plots
	-if you have lots of points you want to add on some sort of interpolation so you can see it
	-transparancy is good to show density if you have more data in one section on another
	-scale size area is also good to show density of the area

Sep 20, 2019: Exploratory Graphics:
	-Anscombe's quartet: 4 x-y data plots that all have the same mean, covariance, ...etc. so they are very easy to combine. all the standard bivariate summary stats are all the same btwn the plots. so its easy to compare
	-so this group of graphs is a really useful way to look at the data to get the general idea about your data and what it looks like and is useful so you dont miss any weird trends. better than doing a lot of tests to check for all the things you can think of
	-scatter plots are generally good, trendlines are usually good when you have too much data
	-confidence intervals are infferential NOT discriptive, bc they are tlaking about the line you drew not the data itself. if we want to see the distribution of the data, we need to do something else
	-violin plots are supposed to be representing the probability density, so you are changing a bunch of points to a distribution/continuous density, it does this by computing the kernal density estimator, which takes a set of points and sum a typically symmertic and smoot function (k) that decreases with distance. (so like a bell shaped curve, gaussian distribution). Then you get a kernal density estimate at a give point x, is the sum over all the locations of the kernal value at that distance. sort of like a histogram but with a sliding window. but they make it smooth off the bat so the output is a smooth non-parametric results.
	-so just like in histograms you need to choose the bin width correctly, the kernal also has a similar issue that you have to choose the bandwidth (width of the gaussian curve) that is good to summarize the data. but both of these are done automatically and huristically
	-so this is why when we do violin plots on not very many points we get weird bumpy violin plots.
	-geom_density_2d() is a 2D kernal to the density, so you get these contour graphs
	-defalt gor this is not super human readable, so the density colours are hard to show bc ppl do not understand the "density levels", also the default colours are shit and makes it harder to compare
	-so with contours you want the inner sections to be more prominant compared to the outter edges so that they look more important and more dense so that there is no misinterpretation.
	-hurrican and trump example (https://eeecon.uibk.ac.at/~zeileis//news/dorian_rainbow/)
	-can put different categories having the contours ontop of eachother, but you should choose fewer contours or use facet, but again these all come with pros and cons
	-also have to think about colour blind and how it would look
	-for big datasets you can use hexes (geom_hex()) and it will go faster
	-for hexes its counts per hex, and its nice bc it allows you to see all the datapoints where as with density it may be missed
	-same general question as how many bars you put in a histogram, you can also choose the number of hexes carefully
	-always the option to manually change the size of the bins/hexes/kernal stuff
	
	Colour:
		-use clear gradients
		-if the map contrasts with the background, zero should match the background
		-if zero has a physical meaning, then go in one direction, red -> white, light -> dark, NOT blue -> white <- blue (no colour in the middle)
		-but if there is a natural "middle" then you can use blue -> white <- red (colour in the middle)
	
	Multi-Variate:
		- if you have lots of data, pairs plots are useful, but they only show the pairwise plots, but this is a good start
		-so you can see how your diff categories are correlated, good summary of the data
		-GGally: similar to ggpairs, so with factors it will do bar plot instead of kernal density plots, and instead of scatter plots, it will make box plots for factors, good overview of the whole dataset
		-double decker plots, spine plots, mosaic plots: for highly multivariate NON-continuous data (categorical)
		-it helps us show and understand the relationships btwn the datapoints, only when its continuous
	
	Diagnostic Graphics:
		-it is important to not do any "snooping" when you are making assumptions before you see if you data fits that model or assumptions or whatever
		-IRL our datasets are never really fitting into our "normal distributions"...etc
		-all we care about is that our data is close enought to normal that our models are ok and good and fine
		-broom package in R, is the extended from tidyverse, i
		-tidy: will give you a coefficent table for a bunch of models in a consistant format with consistant commands, supposed to be unified
		-augment: gives you predictons and inference measures
		-glance: summary of the model
		-so you do all these after you fit your model fo you can look at stuff and plot things
		-qqplots tell us what kind of distribution your data is, heteroskedastic, fat tailed, thin tails...etc
		-so the observed values are y-axis, and expected are on the x. so we can look and see how it would fit with the 1-1 line.
		-so in this example we see that there are fat tails and its symmertical
		-always want to be looking at the distribution of the residuals
		-when fitting a model we need to worry about these things in order:
			-mean model (bias)
			-variance model (heteroscedasticity)
			-distribution (non-normal..etc)
		-residuals should have a constant mean zero
		-you should be looking at the patterns in the residuals among the different categories to see whats wrong and you can fix the model
		-did you leave out covariates and interactions? if its not linear, dont fit linear.
		-think about using polynomal or splines to try and fix non-linear data
		-heteroscidacity: when we plot residuals v fitted we should see a fan. this dosent really work if we have sampling errors or too many points
		-so to fix this we do the square root of fitted and residuals to check and fix those things, here you want the graph to be flat
		-low est plot? they can be wiggly but its usually fine
		-should not use linear reg when doing diagnostics bc you want to see all the variation and weirdness so you can fix it
		-box-cox: we have decided we want to transform the data and its not going to cause problems. so this is a family of transformations (power transformations) x to the power of lambda (where lambda can vary, obviously), so as lambda goes to zero it is a log transformation. so it draws a curve of the goodness of fit or likelyhood of data with confidence intervals of the best lambda so you can pick what transformation you should do
		-this box-cox is said to be a rough guide for you to pick a whole number of lambda (in MASS:boxcox package), bc whole numbers are easier to explain and justify
		-box-cox only handles POSITIVE data
		-can calculate an autocorrelation plot which gives the distribution of the residuals over whatever scale you have (so time or along a chrom or whatever, something with a scae)
		-can group the data from several residuals to get some goodness of fit for a log reg
		-it takes fitted and residuals and groups the data for each of those and tries to see if there is variability of not
		-so you basically bin residuals and it gives you numbers that are a bit easier to interpret
		-can do the same thing but break the data into subsets of categories you already have so that you can then look at the residuals and see whats going on.
		-basically you want to break things down to see if there are patterns that you may be missing
		
		-likelihood profiles: transforming curves to a straight line so you can see if it is quadratic or not
		-if you can see a trend in a trace plot then thats bad, bc your MCMC is not efficently sampling your paramaters
		-trace plots should look like just a band of up and down noise, no trends/patterns

	Sep 24, 2019:What is happening when we do smooth lines
		-loess is like the kernal smoothing, but is essentially fitting a quadratic regression on little pieces of the data, and like kernal smoothing its weighting and sliding the window. weights the points near the middle of the window more
		-computationaly expensive
		-O(N^2) computational notation that says the comp time increases to the number of data points to the squared. so comp time increases with increased num of points
		-has a span thing that is similar to choosing the right bind width...etc, so you can change this to make the curve less or more wiggly
		-mostly defaults are ok
		-if you dont like the results, we usually switch. GAM, additive model, poisson....etc
		-so geom_smooth( method = loess) or change it to whatever, geom has a default but we should specify it always 
		-always make sure your warnings go away bc otherwise you are probs doing something wrong!
		-mgcv fits way too many splices but then is figuring out how wiggly it is and then peanalizes the coefficents so that it brings back to a smooth curve, so the idea with this is that it is a more robst way to choose a curve
		-sometimes with GAM you get lots of lines, which is usually bc it guessed wrong on the number of knots or bins
		-so always just specify everything so that you dont get any warnings
		-with GAMs you can tell it to use a poisson or least squares or whatever, so this is good if you have values that are going below zero when really you should not have that
		-a good way to see whats up with your data is to plot all your x values against each y value? (see multiple continuous predictors, single response)
		-pivot_longer is new and in tidy and a nice way to re-shape your data
		-good way to get a sense of what is happening with all the univariate stuff (so cylinders, carbs, ...etc in the cars dataset) ^
		-again this does not show interactions with other variables, just whats up

		-library "sos" is good for trying to find a command or package or something like that. see lectre notes
		-geom_path puts the points in the order they were in the dataset, where as genom_line assumes you x axis is the predictor variable, so you have to order your points based on the x and y var (so genotype data first, and then environment in the lecture notes)
		-group =1 means that you can do points in diff colours of your groups, and then put a smooth genom line over all the groups if group =1 
	
Oct 1, 2019: Inferential Graphics
	- really important that you are not snooping with your data!
	- what is my model telling me and how should I tell other ppl about this
	- graphs tell stories better than tables do
	- use graphs to illustrate comparisons
	- think consciously about what you units are
	- distinguish btwn scientific and statistical variables 
	- pvalues have a place but there might be better ways to say things
	- its good tobe able to show ppl the data so they can come to the same conclusions as you
	- back to the smoking data
	- danger of fitting a spline model, you can try to find one that give best separation btwn your categories (smoking v non smoking) but then should you be using this model to then do stats on it?
	- this is snooping! and we dont want that! bc then we are not statistically sound
	- before we do any stats we want to make sure that we are protecting against wrong answers, if you have lots of data then you are bound to find something that is sig bc of just the nature of chance
	- so the garden of forking paths is when if you immediatly start looking at your data and then deciding what tests or models you should do, this is inherant snooping, even if you dont mean to
	- and sometime using the models that everyone else does are not always the best things for you to do
	- so in this case, you can hide a certain predictor (smoking), so pick a model for age based only on the age data, so always check residuals of data so you can see whats going on and if it is good
	- so we can look at the units of all these regression coefs and then see if they make sense
	- so remember that your regression coefs have units! liters (in lung capacity) per unit per year, for lung vol v.s. age, or L per unit per inch, for lung vol v.s. height
	- and in the graph they are all on the same graph which is bad
	- and this makes it look like sex is most important factor, but really sex is only more important per inch or per year...etc
	- so you can scale the variables or scale the coefficents so that they are all the same units and then put them on the same scale
	- so we want to multiply the coef by the width, which then gets rid of the units, DW plot is the default to scale quantative coefficents by twice the SD
	- now we have a good pic and units are all the same
	- and we can see that height is the most important predictor in lung capacity, and its easy to see!
	- so in a biological context we know that smoking has some sort of effect on lung capacity, and we care if this graph includes zero bc we could have negative or positive values, which is very often what we want to know
	- so seeing that it inclues zero does not mean that there is not no association
	- so we know smoking is not sig bc we cant tell if it is zero or positive or negative
	- we are using 2 SDs bc then we are not assuming normal distribution with 1.96, or T test...etc
	- coefficent plots where we know the units is a nice way to compare things
	- can get rid of units by scaling the values so these go away, how???
	- so with these graphs the p-values would be low if they are farther away from zero...I think
	- we have been comparing different predictors with the same response variables
	- easest way to standardize is to do it before you fit the model and divide by SD or SE
	- on a log scale can also help to put the values on the same scale
	
	Shape of responses:
		- when things get noisier it is hard to decide how to best present the data
		- if we see there is a positive relationship, or negative or whatever, we want to be able to show that something is sig or not
		- prediction intervals = where you points are likely to be
		- confidence intervals = if you had more data than what you have, this is how sure we are that things should lie within the CI
		- if you ignore the variance at the intercept then you can get CIs for the response (weird cross graph)
		- so the weird cross thing means that the slope could fall anywhere within those regions and we therefore can not say that we are super confident that this is sig!!!
		- so when we put the curves on top of that we can now see that to get the CI curvy ones we see that they take into account for the intercept
		- bc we are on a standard deviation scale thats why it comes out like a cross when we look at the slope CI
		- coefficent plot is most useful when each vriable corresponds to a single statisticl parameter
		- this happens when we have:
			- categorical variable that is binary
			- can assume that we can have a linear predictor and something about a link scale
		- so often we dont worry about the difference btwn scientific vars and stats vars
		- problem comes when we have more than 2 categories (not binary)
		- or if we have a response that is not linear (like lung capacity and age)
		- effects package and ammeans packages that can help with deciding what to do when you want to associate them
		- lots of graphs: supressing variance at the intercept (like before) and we see that there are many different parameters
		- so in this case we would need some sort of overall pval bc there is just no way we could look at this and say what pairs look good or not
		- cant just look at any of the standard pvalues, you are looking at it as a variable, so just religion, country...etc but not distinguishing btwn christian or muslum. so not within the variable
		- link scale is where everything is linear
		- almost always be doing things on link scale for calcilations and for graphing
		- then the original scale for other things
		- for actual data do it on the logit scale?
		- response scale is a probability 
	
	Oct 4, 2019
		- NEVER have + = sig and - = not sig!! in a table
		- also dont put sig or non sig as bold or starred
		- its bc its really hard to tell the difference btwn large effect with small uncertanty, or small effect small uncertanty...etc
		- so it is almost always better to put it in a graph so you can see it clearly in a coefficent plot
		- with coeff plots we are usually dropping the intercept bc usually we dont care about how specific that is
		- like we dont care about mpg on a car with 0 cylynders and 0 displacement..etc, the intercept is a really specific point
		- and from a graphical point of view it often messes with the scale of the graph ( things just wouldnt fit)
		- adding a vertical line with an x intercept of zero, as a reference, so that it is easy to be able to see that things are sig or not
		- can also plot multiple models on the same graph which is hepful sometimes
		- talking about dotwhisker in all the above. this is for creating coeff plots
		- broom package is what pulls out the info we want from the model data (so the coeffs) and then dotwiskers uses it
		- can also just take this out ourselves with broom and then plot it as we like
		- stargazer is a pacakge that will output a nice table for ppl that want the coeff table with * for sig and whatever
		- standard errors are in () and *'s are the pvals, can also produce latex output!!!!!!
		- so again, for a coeff to be sig, we need it and its SEs (or SDs) to be far away from the zero line, NONOVERLAPPING
		- smart way to scale things is to look at SE of the coeffs and then scale, dumb way is to re-scale your data and then re-do your model whould give you the same outcome but the second way takes longer, bc re-fitting the model takes longer
		- the first thing (look at SE and then scale) is what dotwiskers does
		- rescale does the second one, which if you have lots of data and model took a long time, you dont want to scale again
		- so the first way is adjusting the coefficents (scaling them) the second way is scaling the DATA multiple times every time you want to change the scales (the units...etc)
		- if you do want to scale your data, it should be if you are doing something that is simple and not computationally intensive (this happens when you scales say are super super large and then you have numerically issues if you scale the coefs and not the data)
		- if you are doing lin alge, scale data. bc it could fuck up you stuff
		- so when we dont scale, we see that things are not on the same units, or in unintuitive units, and bc our age range is high, we dont really expect the use of contraception per age to change all that much per year per person. so thats why these got super small when we dont scale
		- when you apply scale to a veriable it attaches the old mean and SD so that when you keep doing calcs later, so you should change the matrix output into a vector so that it saves this and does not make you crazy
		- when you use broom and tidy you get a nice tibble of al the info layed out super nicely for you
		- so you can fit different models on the same graph and then compare the coefs from the different models right on the same plot!
		- it automatically dodges!
		- or say if you wanted to drop a predictor and see how it changes the other predictors then its also useful
		- it is also again important to change you legend so that the order of your points are in the same order on the vertical axis, so if you have your data with mostly blue points on top, and orange on bottom, then your legend should also refelct this and be orderd in that way so that it is easier for ppl to see and make connections
		- map() is basically a cleaner replacement for lapply, but this is all in tidy
		- so you can also tell it to do another something like calculate coefs for each thing in the list while you also do something else with the list
		- then you have two dataframes from this map + tidy, and then you want to stick them together with bind_rows(), and you can add a col with a .id inside bind_rows, so you can see what model all those calculations came from (which is used for colouring in plots)
		- unique(term) is to stop it from rearranging alphabetically
		- can also again choose to re-order the terms so that they are from highest to lowest, or order them by effect on the x-axis, again so that ppl can see things more clearly and see what has the most effect or not
		- %+% takes an existing plot but re-draws it for a new data set without changing anything!!!! so helpful!!
		- bc these plots are very horizontal, you can always squash them so that you can make it "prettier" and taking up less white space
		- broom package that handels lots of diff models
		- broom.mixed that hangles more mixed models (lime4, mcmcglm, hlime...etc), BUT you need to make sure you load broom.mixed so that it grabs the right packages
	
	Why Graphs instead of Tables:
		- big tables are hard for ppl AND computers to read! 
		- tabularizer will automatically grab tables from a pdf! super cool. can be problematic sometimes
		- it also has an interactive option where you can select/hilight manually which parts you want it to look at and grab, which apparently works slightly better
		- expand.grid() gives all the combos of different variables (paiwise?)
		- this has its own ordering, so you have to re-order to be what you want
		- R uses regex and if you want an actual backslash you need to do \\, escape the backslash
		- this package is nice, but again you need to spend some time like always, re-shaping your data so it is in a format that you can use in ggplot or whatever
		- tidy has a nice way that you can add in stuff say about the col names, in a nice neat way so that others can see it, can put this in a separate file or you can put it in your code but again, when you do anyting manually it should be OBVIOUS so that ppl can tell and wont accidentally read your code badly
		- geom smooth makes the ribbons look nice
		- if you dont use this, like in his ex in class, then you have to specify all of this in the code

Oct 8, 2019:
	- another reason for why you would do something ordered in alphabetical, is this bc you want to use the graph as a lookup table so ppl can look up values for a certain thing? then alphabetical is fine. otherwise, you should think about the order
	- label_parsed is for doing fancy greek letters and other things, probs helpful with my italics
	- should always cange the facet names so they look pretty and are convenient that they are easier to read
	- Hmisc is a package so that you can add lables and units to variables, BUT it can not work with ggplot :(
	
	Infoviz vs Dataviz:
		- dataviz = how to convey lots of info efficently and honestly, accuate data
		- infoviz = how do we make it catchy, discovery of how big and interesting it is an explore that graphically
		- so I think with infoviz they really want to tell a cool story, even though it may not be totally correct, like the lecture graph about spending and health care and the two that have universal health care are not really that great, so story could be we dont need universal healthcare, but it ignores that you have less datapts for that.
		- say that infoviz is more about recognizing patterns, its more for big datasets with lots of dimensions and you can explore lots
		- dataviz is more about the accuracy of the graph and being able to see values, or extract more info about the datapts
		- dataviz = how well they can communicate auantitative data
		- infoviz is said to be more art
		- lots of discussion about this, and lots of ppl have different opinions and they could vary well be merged
		- so for most of this course and this science, we are thinking as dataviz bc we are often talking to a specific audience who already understands, so we really are dealing with ppl that care about the data
		- infoviz = graphic designers
		- dataviz = stats and science (usually)
		- we often want our visualizations to be:
			-accurate
			-memorable
			-attractive
		-and we can measure this in certain ways:
			-memorability tests
			-quality scores
			-eye-tracking experiments
		- bateman: did actual experiments and did cleaveland like tests to these two graphics, one that is very much more "art" and one that is more of a classic graph, and they said that really ppl were not that much worse when it was art, and they remembered it much better
		- data redundancy is when you have a bar but also lable the exact hight of the bar
		- message redundancy is when you do the same thing but with the categorical variables
		- objectively measuring peoples subjective opinions
		- eye-tracking experiment: least recognizable graphs ppl are already looking all over the place, reading the title..etc. when graphs are recognazible, ppl are able to focus on the center and less looking around. conclude that there is a medium correlation with what you see in the first 10sec vs having longer, redundancy is not bad, titles and labels are important, using recognizable symbols (dinos instead of just dots) helps people connect
		- so having your graphs be reproducable is good bc it means that you can get a new graph with data, but if you are doing infoviz then you are creating art then you cant really put a code to it and therefore you shouldnt have to have a pre-discribed way of creating "art"
	
	Dynamic Graphics:
		- need to think about how someone will actually interact with your graph, will they print it or make it into a pdf that is not dynamic any more?

Oct 11, 2019:
	- so if you have data that is clustered in one section of the graph, then if you ask R to draw it, it will not look great
	- so if you choose 51 points that are evenly spaced across the data so it looks prettier when you plot it (if you are plotting predictions)
	- if you have predictors you want to set up a reference grid
	- here we are talking about very high dimensional models, where you cant possibly draw them all at once
	- so you can average over some dimensions anddrop the dimentions of others, 
	- calculated predicted values everywhere and then avg them
	- or could calculate the avg value of a predictor and ...i dunno
	- jensins ineq= if you ahve a non lin func, the values of the func are not the same as the average of the values, so finding value of x and then conpute avg, or compute all vales at f(x) and then take the avg
	- I SEE
	- ok so you can either calculate the values and then take the average, or you can find the average of your values (x) and then calculate f(avg_x)
	- most ppl calc avg on the linear scale, and then back transform to get the probability
	- but be careful that this is not the avg probability
	- effects, emeans, sjPlot, are all packages in R that will help automate this and help you get a reference grid and calculate the average and stuff

	About Wilkie Book:
		- with the connected scatter plot you can put points or points that are arrows that are evenly spaced in time so you can tell the direction of the plot and how fast things are changing (bc they are the same time pts apart)

Oct 22, 2019:
	Interactive Graphics:
		- we are trying to guide viewers but also trying to get them to explore themselves
		- fine line btwn interactive graphics and animated graphics
		- when does a graphic become an app or a webpage...etc
		- can put more info onto the graph if it is interactive (like the vaccine graphic with the vaccination stuff)
		- can also have a dot on the graph that will then be explained in the caption, in an interactive graphic you can just put this right on the graph but only be visible when you hover over it, so it really can follow the principals of data viz better than just a 2D graphic
		- mostly we want the viewer to zoom in, select certain categories, etc. can do all this without loosing access to other categories, or without presenting too much info
		- can also hilight certain parts of the graph so that viwer can see what they want, so hilighting trendline, but letting the viewer hilight at the points if they want so that they can focus on that and not the trendline 
		
		Gap minder:
		- animated charts, way to make this
		- (https://www.gapminder.org/tools/#$chart-type=bubbles)
		- not R friendly, but this is like the "goal" so you can try to do all these things in R or with google viz
		- so anything that you want to do is basically possible
		- (https://towardsdatascience.com/how-to-build-animated-charts-like-hans-rosling-doing-it-all-in-r-570efc6ba382)
		- plotly: (https://plotly-r.com/)
		- htmlwidgets examples (https://www.htmlwidgets.org/)

		Rpackages:
		- ggvis: is conceptually exciting, but not really useful bc it did not work, so dont really bother using it (package is called plotly)
		- ggplotly
		- googleVis
		- leaflet

	Playing with plotly:
		- can just pipe a ggplot to plotly and it will do some automatic stuff to make it interactive, like zooming and being able to hover over pts and then get extra info for them ..etc. 
		- to make it look nicer will take a bit of playing
		- so you can pass asthetics to ggplot that really do not mean anything for ggplot but then plotly will use this to add extra info so that we can then have this as an interactive bit in the graph
		- can also make animations with ggplotly
		- 

		-reducing sig digits on graphs

Oct 25, 2019:
	Interactive graphs, live play:
		- how to share your interactive graph is with html, or shiny, or you can make a video file and then user can start and stop. but this is really less interactive and more so just showing the story you want
		- cross-talk: can have 2 graphs that are showing different traits and then you can ask ok what are these outliers on this one graph, and were do they fall on another graph that is showing a different trait. (sepal length outliers and where do they fall on the petal graph)
		- so could also have a graph that is a map, and then again can select points that are showing certain categories. and can do it vice versa
		- this was with a crosstalk package
		- with interactive animation basically the programs are drawing a series of plots and then stitching them together with soem sort of set of controls
		- side note: when drawing smooth line or lines connect the points on a scatter plot, you need to be careful that you are not falsly telling ppl that you have data for the sections between the points (tweenr is an R package that will help decide)
		- need to think about:
			1) transitions between frames and how that happens, humans really like smooth transitions, our brians can process it better
			2) syntax and how the graphs should look and what you should put on axis and stuff, what frames you want to show, how specific you want to get
			3) output format: do you want controls, just a vido and loop it?
		- if you have a detailed dataset you might want to do MP4 instead of GIF bc it will only change the actually changing pixls, with GIF you can get it looking a bit weird
		- can also use Java Script that will work on any modern browser
		- Flash format: is not well supported, in linux but also looks like in windows
		- html5 looks like the newest better version of flash
		- you want to use things that have been around for a few years but ahave also been used recently, otherwise you could be using somethign that will be obsolete
		- also with output you need to be thinking about how ppl are going to look at it and what programs they have on their computer, Java is pretty safe and usually will work on everyoens stuff
		- **** can put animation in a pdf but it will only work in acrobat **** thesis easter egg

		3D:
		- 3D is generally bad, but one of the issues is that you can only look at it from 2D on paper
		- but if you have dynamic control, and ppl can spin it like a real model, then that helps!
		- library(rgl) is used to make 3D graphics, talked to the opengl package which is used to actually make the 3D stuff, rgl talks to opengl
		- 
